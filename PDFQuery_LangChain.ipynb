{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Use a local language model using a vector database with query documents for the language model.\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LagChain components \n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "# Support for dataset retrieval with Hugging Face \n",
    "from datasets import load_dataset \n",
    "\n",
    "# Casio use the integracion Astra in LangChain \n",
    "import cassio\n",
    "# PDF reader\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "#environtment \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path of pdf/file \n",
    "pdfreader = PdfReader(\"PyBoy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate \n",
    "#read text from pdf\n",
    "raw_text = \"\"\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "  content = page.extract_text()\n",
    "  if content: \n",
    "    raw_text += content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "db_id = os.getenv(\"ASTRA_DB_ID\")\n",
    "\n",
    "cassio.init(token=token, database_id=db_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community import embeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\",temperature=0.6,base_url=\"http://localhost:11434\")\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\") #This model has very good results when it comes to embedding, at the time of this test it had better results than OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the database and entering data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_vector_store = Cassandra(\n",
    "    embedding=embedding,\n",
    "    table_name = \"qa_mini_demo\",\n",
    "    session=None,\n",
    "    keyspace=None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "#We need to split the text using Character Text Split such that it should not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50 headlines.\n"
     ]
    }
   ],
   "source": [
    "astra_vector_store.add_texts(texts[:50])\n",
    "print(\"Inserted %i headlines.\" % len(texts[:50]))\n",
    "\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model and verify that it uses the data provided from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: \"What is emulation?\"\n",
      "ANSWER: \"Emulation refers to the process of simulating or mimicking the behavior of one system, device, or software on another. In other words, emulation allows a system (such as a computer) to mimic the capabilities and functionality of another system (like an old console or computer), without actually being that system.\n",
      "\n",
      "Emulation can be used for various purposes, such as:\n",
      "\n",
      "1. **Preserving legacy systems**: Emulators can help preserve older systems, software, and hardware by allowing modern devices to run them, even if the original equipment is no longer available.\n",
      "2. **Testing and development**: Emulators can be used in software testing and development to simulate different environments, platforms, or devices, making it easier to test and debug code.\n",
      "3. **Playing classic games**: Emulation of old consoles and computers allows gamers to play classic games on modern systems, without the need for original hardware.\n",
      "4. **Research and education**: Emulators can be used in research and educational settings to study and analyze older systems, software, or hardware.\n",
      "\n",
      "In the context of the project you mentioned, the PyBoy emulator is designed to emulate a Game Boy, allowing users to play classic Game Boy games on modern devices without needing the original hardware.\"\n",
      "\n",
      "FIRTS DOCUMENTS BY RELEVANCE:\n",
      "  [0.7058] \"7.5.1 DMA to OAM . . . . 22\n",
      "7.6 Registers . . . . . . . . . . . . 22\n",
      "7.6.1 LCD Displ ...\"\n",
      "  [0.7058] \"7.5.1 DMA to OAM . . . . 22\n",
      "7.6 Registers . . . . . . . . . . . . 22\n",
      "7.6.1 LCD Displ ...\"\n",
      "  [0.7058] \"7.5.1 DMA to OAM . . . . 22\n",
      "7.6 Registers . . . . . . . . . . . . 22\n",
      "7.6.1 LCD Displ ...\"\n",
      "  [0.6950] \"not be part of the emulation. We have cho-\n",
      "sen to call the emulator \\PyBoy\".\n",
      "For eve ...\"\n"
     ]
    }
   ],
   "source": [
    "firts_question = True\n",
    "while True:\n",
    "  if firts_question:\n",
    "    query_text = input(\"\\nEnter your question (or type 'quit' to exit)\").strip()\n",
    "  else:\n",
    "    query_text = input(\"\\nWhat's your next question (or type  'quit' to exit): \").strip()\n",
    "\n",
    "  if query_text.lower() == \"quit\":\n",
    "    break\n",
    "  if query_text ==\"\":\n",
    "    continue\n",
    "  first_question = False\n",
    "\n",
    "  print(\"\\nQuestion: \\\"%s\\\"\" % query_text)\n",
    "  answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
    "  print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
    "  print(\"FIRTS DOCUMENTS BY RELEVANCE:\")\n",
    "  for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
    "    print(\"  [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
