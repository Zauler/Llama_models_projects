{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a Llama3 model to carry out a study and use case on the categorisation of films into different categories, in this case we are going to focus on the use of promting. We will also use validation, for the output of the model in addition to regular expressions, we will combine the model with another model and we will also use a forced truncation of the words to see if it is effective.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"data/movie_plots_tc.csv\", encoding=\"utf-8\",errors=\"ignore\") as csv_file :\n",
    "    df = pd.read_csv(csv_file, sep=\";\")\n",
    "    \n",
    "#we mix the dataset\n",
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "plots = df_shuffled[\"Plot\"]\n",
    "labels = df_shuffled[\"Genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plot</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Helga Larson Hanson is living in Sweden, but i...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Set in medieval England, the plot concerns the...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joe Holt works for the Armstrong Rubber Goods ...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the film's opening scene, Mark comes to Ale...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All About Lily Chou-Chou follows two boys, Sh?...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Plot   Genre\n",
       "0  Helga Larson Hanson is living in Sweden, but i...   drama\n",
       "1  Set in medieval England, the plot concerns the...  comedy\n",
       "2  Joe Holt works for the Armstrong Rubber Goods ...  comedy\n",
       "3  In the film's opening scene, Mark comes to Ale...   drama\n",
       "4  All About Lily Chou-Chou follows two boys, Sh?...   drama"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drama' 'comedy' 'western']\n",
      "<class 'numpy.ndarray'>\n",
      "2958\n"
     ]
    }
   ],
   "source": [
    "categories = labels.unique()\n",
    "max_length = max([len(s.split()) for s in plots ])\n",
    "print(categories)\n",
    "print(type(categories))\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the model, we are going to try that the model is not creative, but that the answers are as controlled as possible, which is interesting for classification.\n",
    "llm = Ollama(model=\"llama3\",temperature=0, top_p=0.5, repeat_penalty=0.9)\n",
    "\n",
    "#We create a function to facilitate the use of the model\n",
    "def invokeLLama(categories,plots):\n",
    "    task_llm =  f\"\"\" As a film cataloguing specialist, classify the following movie description into a single category. Choose only from the predefined categories listed. Provide your answer as a single category name, without additional comments or explanations.\n",
    "    Available Categories: {categories}\n",
    "    Movie Description: \n",
    "    \"{plots}\"\n",
    "    Category:\n",
    "    \"\"\"\n",
    "    response=llm.invoke(task_llm)\n",
    "    return response\n",
    "\n",
    "def validate_category(categories, output):\n",
    "    categories = categories\n",
    "    if output not in categories:\n",
    "        return \"Unknown\"\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = []\n",
    "for i in range(10):\n",
    "    response=invokeLLama(categories,plots[i])\n",
    "    response = validate_category(categories,response)\n",
    "    yhat.append(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "hits = [1 if yhat[i]==labels[i] else 0 for i in range(len(yhat))]\n",
    "print(\"acc: \", sum(hits)/len(yhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve the model with batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create an example to see the output and to be able to process the data correctly, then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the text, the categories that apply are:\n",
      "\n",
      "1. **Fantasy**: The story features magical elements, such as the jester's ability to switch between different personalities, and the use of magic to manipulate the story's events.\n",
      "2. **Adventure**: The story features a quest, a secret passage, and a battle to save the day, which are all elements of the adventure genre.\n",
      "3. **Romance**: The story features romantic elements, such as the jester's love for the princess and the romantic tension between Rick and Jenny.\n",
      "4. **Comedy**: The story features comedic elements, such as the jester's antics, the misunderstandings, and the slapstick humor.\n",
      "5. **Action**: The story features action-packed scenes, such as the joust, the battle, and the chase.\n",
      "6. **Mystery**: The story features elements of mystery, such as the jester's true identity, the identity of the Black Fox, and the mystery of the secret passage.\n",
      "7. **Sports**: The story features skiing and snowboarding as a central plot point, which is a sport.\n",
      "8. **Coming of Age**: The story features themes of self-discovery and growth, as the characters learn to navigate their relationships and their place in the world.\n"
     ]
    }
   ],
   "source": [
    "#The security of the model we only put it to see how we could validate other types of data, surely this data is not correct, although we could check it if we wanted to.\n",
    "task_llm =  f\"\"\" As a film cataloguing specialist, classify the following movies descriptions into a single category per film. Choose only from the predefined categories listed. Provide your response as a single category name, without additional comments or explanations only a percentage of how confident you are of the category with the format: 'movie position' - ‘category’ - ‘safety %’\n",
    "example of valid output: \n",
    "1 - {categories[0]} - 70%\n",
    "2 - {categories[1]} - 90%\n",
    "3 - {categories[2]} - 55%\n",
    "Available Categories: {categories}\n",
    "\n",
    "Movie Description:\n",
    "1 - \"{plots[0]}\"\n",
    "2 - \"{plots[1]}\"\n",
    "3 - \"{plots[11207]}\"\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "response=llm.invoke(task_llm)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a validation, the outputs will not be good if they don't have a ‘-’.\n",
    "\n",
    "# Output validation\n",
    "from pydantic import BaseModel, field_validator\n",
    "from typing import List\n",
    "\n",
    "# Validate response format - check if it actually contains hyphen (\"-\")\n",
    "class ResponseChecks(BaseModel):\n",
    "    data: List[str]\n",
    "\n",
    "    @field_validator(\"data\")\n",
    "    def check(cls, value):\n",
    "        for item in value:\n",
    "            if len(item) > 0:\n",
    "                assert \"-\" in item, \"String does not contain hyphen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the output pattern to take only the data we are interested in, since the model can generate (as we have seen above) text that we are not interested in, this could be solved with finetuning, but it is not the aim of this example:\n",
    "\n",
    "import re\n",
    "def extract_pattern(response):\n",
    "    pattern = r\"(\\d+)\\s\\-\\s+(\\w+)\"\n",
    "    matches = re.findall(pattern, response)\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the function to facilitate the work of adding batch queries\n",
    "\n",
    "def invokeBatchLLama(categories,plots):\n",
    "    #dinamic part of the prompt\n",
    "    movie_description = \"\\n\".join([f\"{i + 1} - \\'{plots}'\" for i,plots in enumerate(plots)])\n",
    "    task_llm =  f\"\"\" As a film cataloguing specialist, classify the following movies descriptions into a single category per film. Choose only from the predefined categories listed. Provide your response as a single category name, without additional comments or explanations with the format: 'movie position' - ‘category’ \n",
    "    example of valid output: \n",
    "    1 - {categories[0]}\n",
    "    2 - {categories[1]} \n",
    "    3 - {categories[2]} \n",
    "    \n",
    "    Available Categories: \"{categories}\"\n",
    "\n",
    "    Movie Description:\n",
    "    {movie_description}\n",
    "\n",
    "    Remember, never say anything other than position and categories.\n",
    "    Category:\n",
    "    \"\"\"\n",
    "    response=llm.invoke(task_llm)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'comedy'), ('2', 'comedy'), ('3', 'comedy')]\n"
     ]
    }
   ],
   "source": [
    "#Flow test on a small batch size\n",
    "response = invokeBatchLLama(categories,plots[0:3])\n",
    "ResponseChecks(data = [response])\n",
    "response = extract_pattern(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise before using the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a lighter intermediate model to reduce the processing load required to perform this analysis, and by optimising the summary words we can even reduce the processing time. Also we cannot say the right batch size because each description has a very variable length, we are trying to standardise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Program Files\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BartTokenizer\n",
    "\n",
    "def summarise_bart(plots,min_length=10,max_length=35,max_text=700):\n",
    "    #We do this to ensure that we never add more size to the model than it can handle\n",
    "    plots = [\" \".join(plot.split()[0:max_text]) for plot in plots ]\n",
    "    # Initialise the summary pipeline with DistilBART, which is a lighter version of BART.\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "    # Summarise descriptions only if they are larger than the maximum size we allow, otherwise we will save computation.\n",
    "    summarized_plots = [summarizer(plot,min_length=min_length, max_length= max_length)[0]['summary_text'] if len(plot.split())>max_length else plot for plot in plots]\n",
    "\n",
    "    return summarized_plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.6592592592592592\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "Batch_size=45\n",
    "Examples = 180\n",
    "yhat=[]\n",
    "for i in range(0,Examples,Batch_size):\n",
    "    response = summarise_bart(plots[i:i+Batch_size],max_length=35)\n",
    "    response = invokeBatchLLama(categories,response)\n",
    "    ResponseChecks(data = [response])\n",
    "    response = extract_pattern(response)\n",
    "    response = [response[j][1] for j in range(len(response))]\n",
    "    yhat.extend(response)\n",
    " \n",
    "#we see how many times we got it right    \n",
    "hits = [1 if yhat[i]==labels[i] else 0 for i in range(len(yhat))]\n",
    "print(\"acc: \", sum(hits)/len(yhat))\n",
    "print(len(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how it works correctly, we can also see how the longer the summaries are the better the model works, although we can also use a much smaller batch size. We can search for the optimum by performing a function and saving the best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_values_Llama3(Batch_size,Examples,max_length,categories,plots):\n",
    "    yhat=[]\n",
    "    for i in range(0,Examples,Batch_size):\n",
    "        response = summarise_bart(plots[i:i+Batch_size],max_length=max_length)\n",
    "        response = invokeBatchLLama(categories,response)\n",
    "        ResponseChecks(data = [response])\n",
    "        response = extract_pattern(response)\n",
    "        response = [response[j][1] for j in range(len(response))]\n",
    "        yhat.extend(response)\n",
    "    \n",
    "    #we see how many times we got it right    \n",
    "    hits = [1 if yhat[i]==labels[i] else 0 for i in range(len(yhat))]\n",
    "    acc = sum(hits)/len(yhat)\n",
    "    return acc, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Batch_size=30 and max_length=60: 1 validation error for ResponseChecks\n",
      "data\n",
      "  Assertion failed, String does not contain hyphen. [type=assertion_error, input_value=['Here is the classificat...categories listed:\\n\\n'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n",
      "Error with Batch_size=30 and max_length=70: 1 validation error for ResponseChecks\n",
      "data\n",
      "  Assertion failed, String does not contain hyphen. [type=assertion_error, input_value=[\"I'm ready to help. Plea...r the movie summaries.\"], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n",
      "Error with Batch_size=40 and max_length=50: 1 validation error for ResponseChecks\n",
      "data\n",
      "  Assertion failed, String does not contain hyphen. [type=assertion_error, input_value=['I apologize for the mis...se:\\n\\nCategory:\\n    '], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n",
      "Error with Batch_size=40 and max_length=60: 1 validation error for ResponseChecks\n",
      "data\n",
      "  Assertion failed, String does not contain hyphen. [type=assertion_error, input_value=[\"I'm ready to help with ...sition and categories.\"], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n",
      "Error with Batch_size=40 and max_length=70: division by zero\n",
      "Error with Batch_size=50 and max_length=40: 1 validation error for ResponseChecks\n",
      "data\n",
      "  Assertion failed, String does not contain hyphen. [type=assertion_error, input_value=[\"I'm ready to help. Here...se:\\n\\nCategory:\\n    \"], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n",
      "Error with Batch_size=50 and max_length=50: 1 validation error for ResponseChecks\n",
      "data\n",
      "  Assertion failed, String does not contain hyphen. [type=assertion_error, input_value=['I apologize for the mis...ion: 50\\n\\nCategories:'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n",
      "Error with Batch_size=50 and max_length=60: 1 validation error for ResponseChecks\n",
      "data\n",
      "  Assertion failed, String does not contain hyphen. [type=assertion_error, input_value=[\"I'm ready to help with ...ou need anything else.\"], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n",
      "Error with Batch_size=50 and max_length=70: division by zero\n"
     ]
    }
   ],
   "source": [
    "#We are looking for the best combination of parameters to carry out this task.\n",
    "Batch_size=[5,10,20,30,40,50]\n",
    "max_length=[20,30,40,50,60,70]\n",
    "\n",
    "combination = []\n",
    "acc = []\n",
    "for i in Batch_size:\n",
    "    for j in max_length:\n",
    "        try:\n",
    "            accuracy , _ = best_values_Llama3(i, 100, j, categories, plots)\n",
    "            acc.append(accuracy)\n",
    "        except Exception as e:\n",
    "            # Handles error, model does not give correct answers when saturated\n",
    "            print(f\"Error with Batch_size={i} and max_length={j}: {e}\")\n",
    "            acc.append(\"Error\")\n",
    "        finally:\n",
    "            # Adds the combination of parameters used\n",
    "            combination.append((i, j))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The greatest success is:  0.75\n",
      "With the parameters:  (10, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "numeric_results = [x for x in acc if isinstance(x, (int, float))]\n",
    "max_value = max(numeric_results)\n",
    "max_parameters= combination[np.argmax(numeric_results)]\n",
    "print(\"The greatest success is: \", max_value)\n",
    "print(\"With the parameters: \", max_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.9\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#we have to use a very low batch size because some descriptions are really long, saturating the model and not allowing it to generate the response correctly.\n",
    "Batch_size=1\n",
    "Examples = 20\n",
    "yhat=[]\n",
    "for i in range(0,Examples,Batch_size):\n",
    "    response = invokeBatchLLama(categories,plots[i:i+Batch_size])\n",
    "    ResponseChecks(data = [response])\n",
    "    response = extract_pattern(response)\n",
    "    response = [response[j][1] for j in range(len(response))]\n",
    "    yhat.extend(response)\n",
    " \n",
    "#we see how many times we got it right    \n",
    "hits = [1 if yhat[i]==labels[i] else 0 for i in range(len(yhat))]\n",
    "print(\"acc: \", sum(hits)/len(yhat))\n",
    "print(len(yhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the model by analysing the movies one by one with the two models working together we see that the first one is much more accurate, it has almost no faults but the processing time becomes much larger, increasing the computational cost as well as the time. We can see that indeed a bert type model as the one done above with retraining can give better results for such simple tasks, however this is only one way of introducing a model into a data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We remove all the words that do not contribute anything in order to reduce the batch size as much as possible and optimise time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "#here we will have all the words that do not contribute much to a conversation, this is not typical to do in a model that uses transformers, however we want to see how it can work.\n",
    "stopwords_en = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def invokeBatchLLama_2(categories,plots,max_length=50):\n",
    "    \n",
    "    #We do this to ensure that we never add more size to the model than it can handle and remove words that have no value, we also truncate to maximum length so that we can run large batches.\n",
    "    plots = [\" \".join(plot.split()[0:max_length]) for plot in plots if plot.split() not in stopwords_en]\n",
    "    \n",
    "    #dinamic part of the prompt\n",
    "    movie_description = \"\\n\".join([f\"{i + 1} - \\'{plots}'\" for i,plots in enumerate(plots)])\n",
    "    task_llm =  f\"\"\" As a film cataloguing specialist, classify the following movies descriptions into a single category per film. Choose only from the predefined categories listed. Provide your response as a single category name, without additional comments or explanations with the format: 'movie position' - ‘category’ \n",
    "    example of valid output: \n",
    "    1 - {categories[0]}\n",
    "    2 - {categories[1]} \n",
    "    3 - {categories[2]} \n",
    "    \n",
    "    Available Categories: \"{categories}\"\n",
    "\n",
    "    Movie Description:\n",
    "    {movie_description}\n",
    "\n",
    "    Remember, never say anything other than position and categories.\n",
    "    Category:\n",
    "    \"\"\"\n",
    "    response=llm.invoke(task_llm)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.6222222222222222\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "Batch_size=20\n",
    "Examples = 180\n",
    "yhat=[]\n",
    "for i in range(0,Examples,Batch_size):\n",
    "    response = invokeBatchLLama_2(categories,plots[i:i+Batch_size],max_length=55)\n",
    "    ResponseChecks(data = [response])\n",
    "    response = extract_pattern(response)\n",
    "    response = [response[j][1] for j in range(len(response))]\n",
    "    yhat.extend(response)\n",
    " \n",
    "#we see how many times we got it right    \n",
    "hits = [1 if yhat[i]==labels[i] else 0 for i in range(len(yhat))]\n",
    "print(\"acc: \", sum(hits)/len(yhat))\n",
    "print(len(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_values_Llama3_2(Batch_size,Examples,max_length,categories,plots):\n",
    "    yhat=[]\n",
    "    for i in range(0,Examples,Batch_size):\n",
    "        response = invokeBatchLLama_2(categories,plots[i:i+Batch_size],max_length=max_length)\n",
    "        ResponseChecks(data = [response])\n",
    "        response = extract_pattern(response)\n",
    "        response = [response[j][1] for j in range(len(response))]\n",
    "        yhat.extend(response)\n",
    "    \n",
    "    #we see how many times we got it right    \n",
    "    hits = [1 if yhat[i]==labels[i] else 0 for i in range(len(yhat))]\n",
    "    acc = sum(hits)/len(yhat)\n",
    "    return acc, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Batch_size=30 and max_length=45: 1 validation error for ResponseChecks\n",
      "data\n",
      "  Assertion failed, String does not contain hyphen. [type=assertion_error, input_value=[\"I'm ready to help. Plea...sition and categories.\"], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n",
      "Error with Batch_size=30 and max_length=60: 1 validation error for ResponseChecks\n",
      "data\n",
      "  Assertion failed, String does not contain hyphen. [type=assertion_error, input_value=[\"I'm ready to help. The ... Movie\\n\\nPosition: 30\"], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n"
     ]
    }
   ],
   "source": [
    "Batch_size=[5,15,30]\n",
    "max_length=[25,45,60]\n",
    "\n",
    "combination_2 = []\n",
    "acc_2 = []\n",
    "for i in Batch_size:\n",
    "    for j in max_length:\n",
    "        try:\n",
    "            accuracy , _ = best_values_Llama3_2(i, 60, j, categories, plots)\n",
    "            acc_2.append(accuracy)\n",
    "        except Exception as e:\n",
    "            # Handles error, model does not give correct answers when saturated\n",
    "            print(f\"Error with Batch_size={i} and max_length={j}: {e}\")\n",
    "            acc_2.append(\"Error\")\n",
    "        finally:\n",
    "            # Adds the combination of parameters used\n",
    "            combination_2.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The greatest success is:  0.8\n",
      "With the parameters:  (15, 60)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "numeric_results = [x for x in acc_2 if isinstance(x, (int, float))]\n",
    "max_value = max(numeric_results)\n",
    "max_parameters= combination_2[np.argmax(numeric_results)]\n",
    "print(\"The greatest success is: \", max_value)\n",
    "print(\"With the parameters: \", max_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see how the model itself, by cutting the descriptions of the movies and removing the words that do not add value, can perform a fairly accurate accuracy in a fairly efficient and fast way, so we could consider it as a good alternative. However, it is highly dependent on the size of the descriptions and if the descriptions are too long, the model will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to put a current model such as llama3 into a data stream for processing, and we have also seen that we can chain several models together to perform tasks more efficiently. We have also seen ways to process the output with validation and regular expressions. \n",
    "\n",
    "However, the best way to perform this task would be to retrain the model (as we did with the Bert model) to perform this particular task and not use a general, unmodified model. In the following task will focus on re-training the model for this particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
